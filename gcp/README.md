# GCP PyTorch DDP Setup

This folder contains all the scripts and documentation for setting up PyTorch Distributed Data Parallel (DDP) training on Google Cloud Platform.

## ğŸ“ Folder Structure

```
gcp/
â”œâ”€â”€ single-gpu/          # Single GPU setup (working, cost-effective)
â”œâ”€â”€ multi-gpu/           # Multi-GPU setup (requires quota/availability)
â”œâ”€â”€ cleanup.sh           # General cleanup script
â”œâ”€â”€ verify_setup.sh      # Verify GPU and PyTorch setup
â”œâ”€â”€ connect.sh           # Legacy connect script
â”œâ”€â”€ upload.sh            # Legacy upload script
â””â”€â”€ README.md            # This file
```

## ğŸ¯ Current Status

- âœ… **Single GPU Setup**: Working and tested
- âŒ **Multi GPU Setup**: Blocked by resource availability on GCP

## ğŸš€ Quick Start

### For Single GPU (Recommended)
```bash
cd single-gpu
./gcp_setup.sh
```

### For Multi GPU (When resources are available)
```bash
cd multi-gpu
./gcp_setup_multi.sh
```

## ğŸ’° Cost Management

### Check for Running Instances
```bash
gcloud compute instances list
```

### Stop All Instances
```bash
./cleanup.sh
```

### Current Billing Status
- âœ… No instances currently running
- âœ… No ongoing charges
- âœ… Only standard storage costs apply (minimal)

## ğŸ“‹ Next Steps

1. **Immediate**: Use single GPU setup to learn DDP concepts
2. **Later**: Try multi-GPU when GCP resources become available
3. **Alternative**: Consider other cloud providers or local multi-GPU setup

## ğŸ†˜ Support

If you encounter issues:
1. Check the specific README in single-gpu/ or multi-gpu/ folders
2. Ensure no instances are running with `gcloud compute instances list`
3. Use `./cleanup.sh` to clean up any resources

## ğŸ”„ Retry Strategy

Resource availability changes frequently. Try:
- Different times of day
- Different regions
- Different GPU types
- Requesting quota increases
